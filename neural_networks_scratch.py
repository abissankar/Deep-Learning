# -*- coding: utf-8 -*-
"""NEURAL NETWORKS SCRATCH.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1g9052ne5trXvD5neYnUwaVatVZJ56z6k
"""

import  numpy as np

#input
x=np.array([[0,1,1,0],[1,1,0,0],[0,1,0,1]])
x

#to map the input create the output
y=np.array([[1],[0],[1]])
y

#sigmoid func
def sigmoid(x):
  return 1/(1+np.exp(-x))

#derivative sigmoid
def derivative_sigmoid(x):
  return x*(1-x)

#gd needs learning rate
# cycles
epoch=5000
lr=0.1
input_neurons=x.shape[1]
hidden_neurons=3
output_neurons=1

# initializing weights and biases
#wh represents the weights connecting the input layer to the hidden layer
#bh is the bias for the hidden layer
#wo signifies the weights connecting the hidden layer to the output layer,
#bo is the bias for the output layer.
wh=np.random.uniform(size=(input_neurons,hidden_neurons))
bh=np.random.uniform(size=(1,hidden_neurons))
wo=np.random.uniform(size=(hidden_neurons,output_neurons))
bo=np.random.uniform(size=(1,output_neurons))

for i in range(epoch):
  #forward
  hidden_input=np.dot(x,wh)
  hidden_input=hidden_input+bh
  hidden_activation=sigmoid(hidden_input)
  output_layer=np.dot(hidden_activation,wo)
  output=sigmoid(output_layer)
  #backward
  E=y-output
  slope_output=derivative_sigmoid(output)
  slope_hidden=derivative_sigmoid(hidden_activation)
  d_output=E*slope_output
  error_hidden=d_output.dot(wo.T)
  d_hidden=error_hidden*slope_hidden
  wo+=hidden_activation.T.dot(d_output)*lr
  bo+=np.sum(d_output,axis=0,keepdims=True)*lr
  wh+=x.T.dot(d_hidden)*lr
  bh+=np.sum(d_hidden,axis=0,keepdims=True)*lr

output